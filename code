import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix 

from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error

from xgboost import XGBRegressor
from itertools import product
import tensorflow as tf
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        
ds_items = pd.read_csv("/kaggle/input/competitive-data-science-predict-future-sales/items.csv")
ds_sales_train = pd.read_csv("/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv")
ds_submit = pd.read_csv("/kaggle/input/competitive-data-science-predict-future-sales/test.csv")

ds_items.columns, ds_sales_train.columns, ds_submit.columns

#data augmentation

ds_train = ds_train[ds_train['shop_id'].isin(ds_submit['shop_id'].unique())]
ds_train = ds_train[ds_train['item_id'].isin(ds_submit['item_id'].unique())]


ds_combinations = []
cols  = ["date_block_num", "shop_id", "item_id"]
ds_combinations.append(np.array(list(product(ds_train["date_block_num"].unique(),ds_train["shop_id"].unique(),ds_train["item_id"].unique()))))
ds_combinations = pd.DataFrame(np.vstack(ds_combinations), columns = cols)

ds_train = pd.merge(ds_combinations,ds_train, how='left', on=cols)

ds_train.fillna(0, inplace=True)
# handling columns

ds_train = ds_train.join(ds_items.set_index('item_id'), on="item_id").drop(["item_name"], axis=1)
ds_submit = ds_submit.join(ds_items.set_index('item_id'), on="item_id").drop(["item_name"], axis=1)

#creating sets


    ds_submit[col] = ds_submit[col].astype("Sparse[int]")

for col in ds_train.columns:
    ds_train[col] = ds_train[col].astype("Sparse[int]")
    

y_test = ds_test.pop('item_cnt_month').to_numpy()
X_test  = ds_test.sparse.to_coo().tocsr()
X_test

ds_trn = ds_train[ds_train['date_block_num'] != 33].copy()
y_train = ds_trn.pop('item_cnt_month').to_numpy()
X_train = ds_trn.sparse.to_coo().tocsr()
X_train

ds_submit['date_block_num'] = ds_submit['date_block_num'].astype('Sparse[int]')
ds_submit['year'] = ds_test['year'].astype('Sparse[int]')
ds_submit['item_cnt_month_lag_1'] = ds_submit['item_cnt_month_lag_1'].astype('Sparse[int]')

X_submit = ds_submit.sparse.to_coo().tocsr()

#models


    y_pred = model.predict(X_test)
    print(y_pred)
    print(mean_squared_error(y_test, y_pred))
    
#xgboost regressor


mse = 100
for learning_rate in [0.05,0.1, 0.5,1]:
    for max_depth in [5, 10, 25,50]:
            print(learning_rate,max_depth)
            model = XGBRegressor(
                learning_rate = learning_rate,
                max_depth=max_depth,
                booster='gbtree',
                n_estimators=1000,
                min_child_weight=0.5, 
                subsample=0.8,
                sampling_method="uniform",
                colsample_bynode=1,
                colsample_bytree=0.8, 
                eta=0.1)

            model.fit(
                X_train, 
                y_train, 
                eval_metric="rmse", 
                eval_set=[(X_train, y_train), (X_test, y_test)], 
                verbose=True, 
                early_stopping_rounds = 20)

            y_pred = model.predict(X_test)
            if mse>mean_squared_error(y_test, y_pred):
                lr = learning_rate
                mse = mean_squared_error(y_test, y_pred)
                mx_d = max_depth
                best_iter = model.get_booster().best_iteration
            print('XGBoost MSE =', mean_squared_error(y_test, y_pred), "\n")
            
  print(f"Best model had parameters : learning_rate = {lr} and max_depth={mx_d} | MSE = {mse}")
  
  
            
            
        
